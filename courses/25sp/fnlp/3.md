# Lecture 3 - Classification
Fundamentals of Natural Language Processing, 2025 Spring

?> 本课程笔记整理自草鱼，部分补充内容由 deepseek 或 gpt 生成。如有错误，欢迎指正！

!> **重点内容：** 生成式模型和判别式模型

## Generative VS Discriminative
上节课我们接触到了词义消歧的任务，并且尝试用朴素贝叶斯方法解决该问题。词义消歧（Word Sense Disambiguation, WSD）是一个重要的任务，旨在根据上下文确定多义词的正确含义。Naïve Bayes（朴素贝叶斯）是一种常用的统计方法，用于解决WSD问题。其核心思想是基于贝叶斯定理，通过计算给定上下文中每个可能词义的条件概率，选择概率最大的词义作为最终结果。具体公式为：

$$ \text{sense}^* = \arg\max_{\text{sense}} P(\text{sense}) \prod_{\text{word} \in C} P(\text{word}|\text{sense}) $$

其中，\( P(\text{sense}) \) 是词义（sense）的先验概率，\( P(\text{word}|\text{sense}) \) 是在给定词义下，上下文中某个词（word）出现的条件概率。通过最大化这些概率的乘积，模型可以预测最可能的词义。

在**监督学习**的框架下，WSD任务依赖于标注数据。标注数据的形式为 \( (x_1, y_1), (x_2, y_2), \dots, (x_m, y_m) \)，其中 \( x_i \) 是数据样本（例如包含目标词的句子），\( y_i \) 是对应的标签（例如目标词的词义）。监督学习的目标是通过这些标注数据训练一个模型，找到一个函数 \( g \)，使得 \( y = g(x) \)，即模型能够根据输入 \( x \) 预测出正确的词义 \( y \)。

?> **The Goal: find a function g such that** \( y = g(x) \)

从概率的角度来看，这一目标可以形式化为条件概率 \( p(y|x) \)，即给定输入 \( x \) 时输出 \( y \) 的概率分布。具体来说，函数 \( g(x) \) 可以被定义为：

\[ g(x) = \arg\max_{y} p(y|x) \]

也就是说，模型的目标是找到在给定输入 \( x \) 的条件下，最有可能的输出 \( y \)。

为了实现这一目标，机器学习中有两种主要的建模视角：**判别模型（Discriminative Models）** 和 **生成模型（Generative Models）**。这两种方法在如何建模和利用概率分布上有所不同。

---

### **判别模型**
判别模型（Discriminative Models）的核心思想是**直接学习条件概率分布** \( p(y|x) \)。换句话说，判别模型关注的是输入 \( x \) 和输出 \( y \) 之间的直接映射关系，而不需要显式地建模输入的分布 \( p(x) \)。判别模型的目标是通过训练数据直接学习一个决策边界，从而能够对新的输入 \( x \) 进行预测。

- **优点**：
  - 判别模型通常更简单，因为它只需要关注 \( p(y|x) \)，而不需要建模输入的复杂分布。
  - 在分类任务中，判别模型通常表现更好，因为它直接优化了分类边界。
- **典型方法**：
  - 逻辑回归（Logistic Regression）
  - 支持向量机（Support Vector Machines, SVM）
  - 神经网络（Neural Networks）
  - 条件随机场（Conditional Random Fields, CRF）

例如，在词义消歧任务中，判别模型会直接学习给定上下文 \( x \)（包含目标词的句子）时，每个词义 \( y \) 的条件概率 \( p(y|x) \)，然后选择概率最大的词义作为预测结果。

---

### **生成模型**
生成模型（Generative Models）的核心思想是**先学习联合概率分布** \( p(x, y) \)，然后通过**贝叶斯定理**推导出条件概率 \( p(y|x) \)。生成模型不仅关注输入和输出之间的关系，还试图建模输入数据 \( x \) 的分布 \( p(x) \)。通过学习联合分布，生成模型可以生成新的数据样本。

- **优点**：
  - 生成模型能够生成新的数据，因此在某些任务（如文本生成、图像生成）中非常有用。
  - 它可以更好地处理缺失数据或噪声数据。
- **典型方法**：
  - 朴素贝叶斯（Naïve Bayes）
  - 隐马尔可夫模型（Hidden Markov Models, HMM）
  - 高斯混合模型（Gaussian Mixture Models, GMM）

例如，在词义消歧任务中，生成模型会先学习联合分布 \( p(x, y) \)，即上下文 \( x \) 和词义 \( y \) 同时出现的概率，然后通过贝叶斯定理计算条件概率 \( p(y|x) = \frac{p(x, y)}{p(x)} \)，最后选择最可能的词义。

---

?> **两种视角的对比** 判别模型更关注输入和输出之间的直接关系，适合分类任务，尤其是在数据量较大时表现更好。生成模型则更全面地建模数据的分布，适合生成任务或需要处理数据分布的场景，但在分类任务中可能因为建模复杂度较高而表现不如判别模型。在实际应用中，选择哪种模型取决于具体任务的需求。

