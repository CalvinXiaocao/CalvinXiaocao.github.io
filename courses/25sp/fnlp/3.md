# Lecture 3 - Classification
Fundamentals of Natural Language Processing, 2025 Spring

?> 本课程笔记整理自草鱼，部分补充内容由 deepseek 或 gpt 生成。如有错误，欢迎指正！

!> **重点内容：** 生成式模型和判别式模型、词袋模型和 TF-IDF weighting、log-linear 模型的定义及目标函数推导、正则化

## Generative VS Discriminative
上节课我们接触到了词义消歧的任务，并且尝试用朴素贝叶斯方法解决该问题。词义消歧（Word Sense Disambiguation, WSD）旨在根据上下文确定多义词的正确含义，Naïve Bayes（朴素贝叶斯）是一种常用的统计方法，其核心思想是基于贝叶斯定理，通过计算给定上下文中每个可能词义的条件概率，选择概率最大的词义作为最终结果。具体公式为：

$$ \text{sense}^* = \arg\max_{\text{sense}} P(\text{sense}) \prod_{\text{word} \in C} P(\text{word}|\text{sense}) $$

其中，\( P(\text{sense}) \) 是词义（sense）的先验概率，\( P(\text{word}|\text{sense}) \) 是在给定词义下，上下文中某个词（word）出现的条件概率。通过最大化这些概率的乘积，模型可以预测最可能的词义。

在**监督学习**的框架下，WSD任务依赖于标注数据。标注数据的形式为 \( (x_1, y_1), (x_2, y_2), \dots, (x_m, y_m) \)，其中 \( x_i \) 是数据样本（例如包含目标词的句子），\( y_i \) 是对应的标签（例如目标词的词义）。监督学习的目标是通过这些标注数据训练一个模型，找到一个函数 \( g \)，使得 \( y = g(x) \)，即模型能够根据输入 \( x \) 预测出正确的词义 \( y \)。

?> **The Goal: find a function g such that** \( y = g(x) \)

从概率的角度来看，这一目标可以形式化为条件概率 \( p(y|x) \)，即给定输入 \( x \) 时输出 \( y \) 的概率分布。具体来说，函数 \( g(x) \) 可以被定义为：

$$ g(x) = \arg\max_{y} p(y|x) $$

也就是说，模型的目标是找到在给定输入 \( x \) 的条件下，最有可能的输出 \( y \)。

为了实现这一目标，机器学习中有两种主要的建模视角：**判别模型（Discriminative Models）** 和 **生成模型（Generative Models）**。这两种方法在如何建模和利用概率分布上有所不同。我们先从宏观上对比一下这两种模型。

---

**判别模型**

判别模型（Discriminative Models）的核心思想是**直接学习条件概率分布** \( p(y|x) \)。换句话说，判别模型关注的是输入 \( x \) 和输出 \( y \) 之间的直接映射关系，而不需要显式地建模输入的分布 \( p(x) \)。判别模型的目标是通过训练数据直接学习一个决策边界，从而能够对新的输入 \( x \) 进行预测。

- **优点**：
  - 判别模型通常更简单，因为它只需要关注 \( p(y|x) \)，而不需要建模输入的复杂分布。
  - 在分类任务中，判别模型通常表现更好，因为它直接优化了分类边界。
- **典型方法**：
  - 逻辑回归（Logistic Regression）
  - 支持向量机（Support Vector Machines, SVM）
  - 神经网络（Neural Networks）
  - 条件随机场（Conditional Random Fields, CRF）

例如，在词义消歧任务中，判别模型会直接学习给定上下文 \( x \)（包含目标词的句子）时，每个词义 \( y \) 的条件概率 \( p(y|x) \)，然后选择概率最大的词义作为预测结果。

---

**生成模型**

生成模型（Generative Models）的核心思想是**先学习联合概率分布** \( p(x, y) \)，然后通过**贝叶斯定理**推导出条件概率 \( p(y|x) \)。生成模型不仅关注输入和输出之间的关系，还试图建模输入数据 \( x \) 的分布 \( p(x) \)。通过学习联合分布，生成模型可以生成新的数据样本。

- **优点**：
  - 生成模型能够生成新的数据，因此在某些任务（如文本生成、图像生成）中非常有用。
  - 它可以更好地处理缺失数据或噪声数据。
- **典型方法**：
  - 朴素贝叶斯（Naïve Bayes）
  - 隐马尔可夫模型（Hidden Markov Models, HMM）
  - 高斯混合模型（Gaussian Mixture Models, GMM）

例如，在词义消歧任务中，生成模型会先学习联合分布 \( p(x, y) \)，即上下文 \( x \) 和词义 \( y \) 同时出现的概率，然后通过贝叶斯定理计算条件概率 \( p(y|x) = \frac{p(x, y)}{p(x)} \)，最后选择最可能的词义。

![alt text](image-4.png 'size=60%')

?> **两种视角的对比** 判别模型更关注输入和输出之间的直接关系，适合分类任务，尤其是在数据量较大时表现更好。生成模型则更全面地建模数据的分布，适合生成任务或需要处理数据分布的场景，但在分类任务中可能因为建模复杂度较高而表现不如判别模型。在实际应用中，选择哪种模型取决于具体任务的需求。

下面我们来详细介绍这两种模型。

---

### Generative Models

生成模型（Generative Models）的核心思想是通过建模联合概率分布 \( p(x, y) \) 来学习输入数据 \( x \) 和类别 \( y \) 之间的关系。与判别模型直接学习条件概率 \( p(y|x) \) 不同，生成模型从联合分布 \( p(x, y) \) 出发，利用贝叶斯定理推导出条件概率 \( p(y|x) \)。以下是生成模型的详细推导过程及其背后的逻辑：

---

**1. 联合概率分布 \( p(x, y) \)**

生成模型首先关注的是联合概率分布 \( p(x, y) \)，即输入数据 \( x \) 和类别 \( y \) 同时出现的概率。根据概率的链式法则，联合分布可以分解为：

$$ p(x, y) = p(y) \cdot p(x|y) $$

其中：
- \( p(y) \) 是类别 \( y \) 的先验概率，表示类别 \( y \) 在数据中出现的概率。
- \( p(x|y) \) 是条件概率，表示在类别 \( y \) 下，数据 \( x \) 的生成概率。

---

**2. 利用贝叶斯定理推导条件概率 \( p(y|x) \)**

生成模型的最终目标是计算条件概率 \( p(y|x) \)，即在给定输入数据 \( x \) 的情况下，类别 \( y \) 的概率。根据贝叶斯定理，条件概率 \( p(y|x) \) 可以通过联合分布 \( p(x, y) \) 和边缘分布 \( p(x) \) 计算得到：

$$ p(y|x) = \frac{p(x, y)}{p(x)} $$

其中，边缘分布 \( p(x) \) 可以通过对所有可能的类别 \( y' \) 求和得到：

$$ p(x) = \sum_{y'} p(y') \cdot p(x|y') $$

因此，条件概率 \( p(y|x) \) 可以进一步表示为：

$$ p(y|x) = \frac{p(y) \cdot p(x|y)}{\sum_{y'} p(y') \cdot p(x|y')} $$

---

**3. 预测函数 \( g(x) \) 的推导**

生成模型的预测函数 \( g(x) \) 的目标是找到在给定输入 \( x \) 时，最可能的类别 \( y \)。根据最大后验概率（MAP）准则，预测函数可以表示为：

$$ g(x) = \arg\max_{y} p(y|x) $$

将条件概率 \( p(y|x) \) 的表达式代入，可以得到：

$$ g(x) = \arg\max_{y} \frac{p(y) \cdot p(x|y)}{\sum_{y'} p(y') \cdot p(x|y')} $$

注意到分母 \( \sum_{y'} p(y') \cdot p(x|y') \) 对于所有类别 \( y \) 都是相同的，因此在最大化过程中可以忽略。于是，预测函数简化为：

$$ g(x) = \arg\max_{y} p(y) \cdot p(x|y) $$

进一步地，由于 \( p(y) \cdot p(x|y) = p(x, y) \)，预测函数也可以表示为：

$$ g(x) = \arg\max_{y} p(x, y) $$

---

**4. 生成模型的最终形式**

综上所述，生成模型的预测函数最终可以写成以下两种等价形式：

1. **基于联合分布的形式**：
   $$ g(x) = \arg\max_{y} p(x, y) $$

2. **基于先验和条件分布的形式**：
   $$ g(x) = \arg\max_{y} p(y) \cdot p(x|y) $$

这两种形式都体现了生成模型的核心思想：通过建模联合分布 \( p(x, y) \) 或分解为 \( p(y) \) 和 \( p(x|y) \)，来推导条件概率 \( p(y|x) \)，并最终用于预测。

?> 然而，很多情况下，\( p(x|y) \) 是很难直接计算的，因此生成模型通常会做一些假设，**如朴素贝叶斯中的特征条件独立性假设**，来简化计算。

> 例：图书分类

**问题描述**

假设我们有一个书籍分类任务，目标是预测一本书的类别 \( Y \)（例如侦探小说、科幻小说等），基于书中的词汇 \( X \)。我们需要建模以下两个概率分布：
1. **条件概率** \( P(X|Y) \)：在给定书籍类别 \( Y \) 的情况下，书中词汇 \( X \) 的分布。例如，侦探小说中可能会频繁出现“谋杀”、“侦探”等词汇。
2. **后验概率** \( P(Y|X) \)：这是我们最终感兴趣的，即在给定词汇 \( X \) 的情况下，书籍类别 \( Y \) 的概率。

---

**贝叶斯建模的假设**

为了建模这个问题，我们需要做一些假设，并引入概率分布来描述数据生成过程。

**书籍类别的生成**

假设书籍类别 \( Y \) 是从一个多项式分布（Multinomial Distribution）中生成的，其参数为 \( \gamma \)：

$$ Y \sim \text{Multinomial}(\gamma) $$

其中，\( \gamma \) 是一个概率向量，表示每个类别的先验概率。举个例子，假设有侦探、科幻、历史三类小说，他们在总体中出现的概率分别为 \( \gamma = [0.35, 0.4, 0.25] \)。这就是概率向量。

?> 💡 **多项式分布**（Multinomial Distribution）是二项分布的推广，用于描述在多次独立试验中，多个类别（或结果）出现次数的概率分布。假设每次试验有 \( k \) 种可能的结果，每种结果出现的概率分别为 \( p_1, p_2, \dots, p_k \)，则在 \( n \) 次试验中，**每种结果出现次数的联合概率**分布就是多项式分布。它常用于自然语言处理中的词频统计、分类任务中的类别分布建模等场景。

**词汇的生成**

在给定书籍类别 \( Y \) 的情况下，书中的词汇 \( X \) 是从一个条件分布 \( p(X|\theta_Y) \) 中生成的。这里，\( \theta_Y \) 是类别 \( Y \) 的参数，描述了该类别的词汇分布。

$$ X|Y \sim p(X|\theta_Y) $$

例如，对于侦探小说，\( \theta_Y \) 可能表示“谋杀”、“侦探”等词汇的高概率，而科幻小说可能更倾向于“太空”、“外星人”等词汇。

**先验分布**

为了完成贝叶斯建模，我们需要为参数 \( \gamma \) 和 \( \theta_Y \) 指定先验分布：
- 对于 \( \gamma \)，我们通常使用狄利克雷分布（Dirichlet Distribution）作为先验，因为它是多项式分布的共轭先验：

  $$ \gamma \sim \text{Dirichlet}(\beta) $$

  其中，\( \beta \) 是狄利克雷分布的参数。

- 对于每个类别的词汇分布参数 \( \theta_Y \)，我们假设它们是从某个先验分布 \( p(\theta) \) 中独立同分布（iid）生成的：

  $$ \theta_1, \theta_2, \dots \sim p(\theta) $$

?> **共轭先验**: 在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。（要求后验分布与先验分布是同类分布，不要求似然函数分布相同。）

---

**联合概率分布**

基于以上假设，我们可以写出联合概率分布 \( p(X, Y | \gamma, \Theta) \)，其中 \( \Theta = \{\theta_1, \theta_2, \dots\} \) 是所有类别的词汇分布参数。联合分布可以分解为：

$$ p(X, Y | \gamma, \Theta) = p(X | Y, \Theta) \cdot p(Y | \gamma) $$

其中：
- \( p(Y | \gamma) \) 是书籍类别 \( Y \) 的生成概率，由多项式分布描述。
- \( p(X | Y, \Theta) \) 是在给定类别 \( Y \) 和参数 \( \Theta \) 的情况下，词汇 \( X \) 的生成概率。

---

**后验概率**

我们的最终目标是计算后验概率 \( P(Y | X) \)，即在给定词汇 \( X \) 的情况下，书籍类别 \( Y \) 的概率。根据贝叶斯定理，后验概率可以通过联合分布和边缘分布计算得到：

$$ P(Y | X) = \frac{p(X, Y | \gamma, \Theta)}{p(X | \gamma, \Theta)} $$

其中，边缘分布 \( p(X | \gamma, \Theta) \) 可以通过对所有可能的类别 \( Y \) 求和得到：

$$ p(X | \gamma, \Theta) = \sum_{Y} p(X, Y | \gamma, \Theta) $$

?> Bonus 拓展：**Latent Dirichlet Allocation** https://zhuanlan.zhihu.com/p/309419680

---

### Discriminative Models

判别模型直接建模条件概率 \( p(y|x) \)，即给定观测数据 \( x \) 时，未观测类别标签 \( y \) 的概率分布。与生成模型不同，判别模型不关注数据的生成过程，而是专注于输入 \( x \) 和输出 \( y \) 之间的直接映射关系。

**特点**：

1. **直接建模条件概率**：判别模型直接学习 \( p(y|x) \)，而不需要显式建模联合分布 \( p(x, y) \)。
2. **无需独立性假设**：判别模型不需要对输入特征之间的独立性做出假设，因此可以更灵活地处理复杂数据。
3. **易于融入多种特征**：判别模型可以轻松引入各种特征（如上下文信息、交互特征等），从而提高模型表现。
4. **通常精度更高**：由于直接优化分类边界，判别模型在分类任务中通常表现更好。
5. **可能过拟合**：如果数据量不足或模型复杂度过高，判别模型容易过拟合。

**典型方法**：

- **概率模型**：最大熵模型（Maximum Entropy）、条件随机场（Conditional Random Field, CRF）、逻辑回归（Logistic Regression）。
- **非概率模型**：支持向量机（SVM）、感知机（Perceptron）等。

**实现方式**：

判别模型通过优化条件概率 \( p(y|x) \) 的参数，直接学习输入 \( x \) 到输出 \( y \) 的映射关系。常见的优化方法包括最大似然估计（MLE）和梯度下降等。

下面，我们以 Discriminative Models 为例，介绍一个经典的判别模型：**逻辑回归（Logistic Regression）**。

## Before Classification: Feature Representation

1. **特征定义**：
   - 特征是描述观察数据 \( x \) 的某些方面的证据，通常与预测标签 \( y \) 相关。
   - 特征是一个关于 \( x \) 和 \( y \) 的函数，\( f_i(x, y) \in \mathbb{R} \)。
   - 通常，特征是一个二元或指示函数，例如在词义消歧（WSD）中：
     $$
     f_i = 
     \begin{cases} 
     1 & \text{如果 } w_{-1} = \text{transfer 且 } y = \text{FINANCIAL}, \\
     0 & \text{否则}
     \end{cases}
     $$

2. **特征向量**：
   - 如果有 \( m \) 个方面来描述一个实例，即 \( m \) 个特征，那么每个实例 \( (x, y) \) 的特征向量为：
     $$
     [f_1(x, y), f_2(x, y), f_3(x, y), \ldots, f_m(x, y)]
     $$
   - 例如，特征向量可能看起来像：\( [1, 0, 0, \ldots, 1, 0] \)。

![alt text](image-9.png ':size=60%')


**在 WSD 中的示例**

![alt text](image-10.png ':size=60%')

**在新闻分类中的示例**

$$
f_{around, Sp} = 1, f_{soccer, Sp} = 1, f_{Portugal, Sp} = 0, \cdots, f_{Portugal, notSp} = 1
$$

我们可以把特征向量排列起来，$[1,1,1,1,0,0,\dots,1]$。或者，我们也可以使用频率：$[1,2,1,4,\dots,90,0,0]$。或者，我们还可以用 **TF-IDF** 方式赋予权重。

### TF-IDF Weighting

一个单词 $t$ 在文档 $d$ 中的重要性可以通过词频-逆文档频率进行衡量。具体而言：

$$
TF_{t, d} = \begin{cases}1 + \log_{10} count(t, d) &\text{if } count(t, d) > 0 \\ 0 & \text{otherwise}\end{cases}
$$

$$
IDF_t = log_{10} \frac{N}{DF_t}
$$

代表总文件数除以包含该词语的文件数目。**往往需要强调是在一个事先公开的高质量语料库里计算，而不是训练集中计算。**

$$
\text{TF-IDF} = TF_{t,d} \times IDF_t
$$

----

?> 例：我们现在有三个文档构成的语料库。**文档1**: "The cat sat on the mat" **文档2**: "The dog sat on the log" **文档3**: "The cat and the dog are friends" 现在计算文档1的 TF-IDF 向量。

**第一步：计算词频（TF）**

词频（Term Frequency, TF）表示一个词在文档中出现的频率。这里我们使用对数化的公式：

$$
TF_{t, d} = \begin{cases}1 + \log_{10} count(t, d) &\text{if } count(t, d) > 0 \\ 0 & \text{otherwise}\end{cases}
$$

以文档1 "The cat sat on the mat" 为例：

- 分词结果：["the", "cat", "sat", "on", "the", "mat"]
- 词频统计：
  - the: 2
  - cat: 1
  - sat: 1
  - on: 1
  - mat: 1

计算 TF：

- the: \(1 + \log_{10}(2) \approx 1 + 0.301 = 1.301\)
- cat: \(1 + \log_{10}(1) = 1 + 0 = 1\)
- sat: 1
- on: 1
- mat: 1

**第二步：计算逆文档频率（IDF）**

逆文档频率（Inverse Document Frequency, IDF）衡量一个词的普遍重要性。公式为：

$$
IDF_t = \log_{10} \left( \frac{N}{DF_t} \right)
$$

其中：
- \(N\) 是文档总数（这里是3）
- \(DF_t\) 是包含词 \(t\) 的文档数量

首先统计每个词的 DF：

!> 正常应该另给好的语料库的！但这里我们就简化了，直接把它当做计算 idf 的语料库。

- the: 出现在所有3个文档 → DF=3
- cat: 出现在文档1和文档3 → DF=2
- sat: 出现在文档1和文档2 → DF=2
- on: 出现在文档1和文档2 → DF=2
- mat: 只在文档1出现 → DF=1

计算 IDF：

- the: \(\log_{10}(3/3) = \log_{10}(1) = 0\)
- cat: \(\log_{10}(3/2) \approx \log_{10}(1.5) \approx 0.176\)
- sat: \(\log_{10}(3/2) \approx 0.176\)
- on: \(\log_{10}(3/2) \approx 0.176\)
- mat: \(\log_{10}(3/1) \approx \log_{10}(3) \approx 0.477\)

**第三步：计算 TF-IDF**

TF-IDF 是 TF 和 IDF 的乘积：

$$
\text{TF-IDF}_{t,d} = TF_{t,d} \times IDF_t
$$

以文档1中的 "cat" 为例：

- TF("cat", 文档1) = 1
- IDF("cat") ≈ 0.176
- TF-IDF = \(1 \times 0.176 = 0.176\)

再以文档1中的 "mat" 为例：

- TF("mat", 文档1) = 1
- IDF("mat") ≈ 0.477
- TF-IDF = \(1 \times 0.477 = 0.477\)

**完整示例：文档1的 TF-IDF 向量**

文档1 "The cat sat on the mat" 的词和 TF-IDF 值：

- the: \(1.301 \times 0 = 0\) （"the" 在所有文档中出现，IDF=0，因此 TF-IDF=0）
- cat: \(1 \times 0.176 = 0.176\)
- sat: \(1 \times 0.176 = 0.176\)
- on: \(1 \times 0.176 = 0.176\)
- mat: \(1 \times 0.477 = 0.477\)

其他词的 TF-IDF 为 0（因为未出现在文档1中）。

-----

### Bag Of Words 词袋模型
通过上述的方式，我们得到了一种简单的特征：统计文档每个词出现的频率。用独热向量、频率或 TF-IDF 值都可以。我们把它称为词袋模型。

而对于有些单词，例如 a/an/the 等经常出现的词汇，它对任务的贡献并不大。我们引入停用词表 (the stop-word list)，将它们进行忽略。

?> 上面说的特征分明是一个二元函数 $f(x,y)$ 啊，为什么这里没有 $y$ 了呢？一种解释：词袋模型也是一种特征，这样的特征基于文本的本身，仅依赖输入，是特征的一种简单形式。

-----

## Feature Based Discriminative Models
我们首先尝试一个线性的分类器：$\lambda_{f(x, y)} f(x, y)$，其中 $\lambda$ 是权重。我们建立了一个从 $f(x, y)$ 到 $y$ 的线性映射。那么我们如何计算类别呢？只需要先算每个 instance $x$ 的 score：$$score(x,y) = \sum_i \lambda_{f_i(x, y)} f_i(x, y)$$ 这样的话，分类器会取使得 score 最大的 $y$ 作为 $y^*$。

那么，这些 $\lambda$ 该如何确定呢？我们的关键是，对于不同的特征，确定好特定的 $\lambda$。常见的方法有：
- 感知机算法
- 基于 Margin 的模型 （如SVM）
- 指数集的模型

这里，我们重点讲解 Log-Linear Models。

### Log-Linear Models
对于一对数据 $(x, y)$，我们在意的是：$$p(y \mid x) = \frac{\exp \sum_i \lambda_{f_i(x, y)} f_i(x, y)}{\sum_{y'}exp\sum_i \lambda_{f_i(x, y')} f_i(x, y')} = \frac{\exp (\bm \lambda \cdot \bm f(x, y))}{\sum_{y'}\exp (\bm \lambda \cdot \bm f(x, y'))}$$

`其实无非就是一个 softmax 而已。`

那么我们如何确定里面的 $\bm \lambda$ 呢？这时极大似然估计就可以派上用场了。

**构建似然函数**

首先构建似然函数，也就是训练数据发生的概率。$$L(\bm \lambda) = \prod_k p(y_k \mid x_k; \bm \lambda) = \prod_k \frac{\exp (\bm \lambda \cdot \bm f(x_k, y_k))}{\sum_{y'}\exp (\bm \lambda \cdot \bm f(x_k, y'))}$$

**求对数似然**

$$LL(\bm \lambda) = \sum_k \log p(y_k \mid x_k; \bm \lambda) = \sum_k \bm \lambda \cdot \bm f(x_k, y_k) - \sum_k \log \sum_{y'} \exp(\bm \lambda \cdot \bm f(x_k, y'))$$

我们的目标是，最大化对数似然函数。

**求导并让导数为0**

$$\frac{\partial LL(\bm \lambda)}{\partial \lambda_{f_i(,)}} = \sum_k f_i (x_k, y_k) - \sum_k \sum_{y'}f_i(x_k, y')p(y' \mid x_k; \bm \lambda)$$

第一项：Empirical Counts，第二项：Expected Counts。如果对数似然函数最大，那么导数应该尽可能为0。也就是让 Empirical Counts 尽可能去 match Expected Counts。

### Gradient Ascend Methods
我们想训练使得对数似然函数最大，可以用梯度上升的方式。梯度上升算法如下：
- 初始化所有的 $\lambda$ 为0
- 重复下面的过程一直到收敛
  - 计算 $\Delta = \frac{\partial LL(\bm \lambda)}{\partial \bm \lambda}$
  - 计算 $\beta_* = \argmax_\beta LL(\bm \lambda + \beta \cdot \Delta)$
  - $\bm \lambda \gets \bm \lambda + \beta_* \cdot \Delta$

问题？这样的算法太慢了。我们引入**随机梯度上升法**。

- 初始化所有的 $\lambda$，学习率 $\alpha$，epoch 数目 $T$
- 对于 $t \in \{1, 2, \dots, T\}$：
  - 对训练数据进行重排 $\pi$
  - 对于每一个 $i \in \{1, 2, \dots, k \}$：
    - 计算 $\Delta = \frac{\partial LL(\bm \lambda)}{\partial \bm \lambda}$
    - $\bm \lambda \gets \bm \lambda + \alpha \cdot \Delta$

如果我们把上面的目标函数（对数似然）取负，那么对应的就是交叉熵损失函数。

### Regularization
训练结果可能出现过拟合的现象。比如我们有一个特征$$f_1(x, y)=\begin{cases} 1 & \text{if x contains NFL and y is Sports} \\ 0 & \text{otherwise} \end{cases}$$ 在训练的时候，语料中每次 NFL 出现的时候文本都是 Sports。那么很可能训练后 $\lambda_1 \to +\infty$。这样模型就过拟合了。为了避免这样的情况发生，我们引入正则化项。

我们把损失函数改成以下的形式：

$$
\argmin_{\bm \lambda} loss(\bm \lambda) + \alpha ||\bm \lambda||_p
$$

当 $p=1$ 时，称为 L-1 正则化，它会鼓励特征的稀疏性。当 $p=2$ 时，称为 L-2 正则化，它会防止某个特征对应的权重太大。

## Evaluation
在训练时，我们会把数据划分成训练集和测试集两部分。训练集用于训练，获得参数，测试集用来评估效果。比如，在使用 Log-linear 分类器时，我们用训练集来估计 $\bm \lambda$，用测试集来估计 $F-1$ 分数。

现在的问题：如何在众多特征中选择合适的特征？我们要再选取出来一部分数据作为 validation test 验证集。在验证集上找到效果最好的一组特征，用这组特征进行训练，最终在测试集上评估。

还有一种方式：K-fold 交叉验证。在 $X_{1:K} - X_i$ 上训练，$X_i$ 上做验证。依次更换。最终取多次验证的 $F_1$ 的平均值作为最终 $F_1$。

![alt text](image-37.png ':size=70%')
