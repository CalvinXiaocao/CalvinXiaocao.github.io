# Lecture 5 - Sequence Tagging I - Hidden Markov Models
Fundamentals of Natural Language Processing, 2025 Spring

?> 本课程笔记整理自Freefizing & 草鱼。

!> **重点内容：** 序列标注的概念、隐马尔可夫模型、Viterbi 算法

## Review: Generative & Discriminative

**Generative Model - Naïve Bayes**

生成式模型适用于多个相关决策、背景知识已知、数据生成与补全。

**Discriminative Model - Log-Linear Model**

判别式模型适用于大量标注数据、单一目标决策。

-----

## New Problem: Sequence Tagging Problem

> Definition: 序列标注

* 输入：一串词构成的句子
* 任务：标注这些词哪个“有用”
* 输出：输出是一个标记序列或状态序列

序列标注（Sequence Tagging）是NLP中最基础的任务，应用十分广泛，如分词、词性标注（POS tagging）、命名实体识别（Named Entity Recognition，NER）、关键词抽取、语义角色标注（Semantic Role Labeling）、槽位抽取（Slot Filling）等。

我们会以**词性标注**（POS tagging）为例进行详细介绍。

-----

### POS Tagging
* 输入：一句话
* 同时给定：所有的词性集合（确定性集合）与标点
* 输出：每个词的词性标注

### 命名实体识别
**Noun phrase chunking**: 从一段词中找出基本的名词短语。

?> 例：[China Mobile] is [a communication giant] in [east Asia].

我们创造三种标签：
* B: 名词短语的开始 Beginning
* I: 名词短语的继续
* O: 其他词

**Named entity recognition** (NER): 命名实体识别，找到句中所有的“命名实体”（例如人物，地点，组织等）。标注也可以用上面的方式。

?> 问题：1. “北京市政府”该怎么标？北京 & 北京市 & 北京市政府…… 2. 有些实体可以不连续。

**中文分词**

把一些字分成词

### 两种类型的单词
* **Closed class words**
  * 封闭类词是指那些数量相对固定、不易增加新成员的词类。它们通常是语法功能词，用于构建句子的结构或表达语法关系。
  * 例如介词，连词，代词，冠词等。
* **Open class words**
  * 开放类词是指那些数量可以不断扩展、容易增加新成员的词类。它们通常承载具体的词汇意义。
  * 例如名词，动词，形容词，副词

**Tagset**（标记集）是自然语言处理（NLP）中的一个重要概念，指的是用于标注语言数据的标签集合。这些标签通常用来表示词汇或句子的语法、语义或其他语言学特征。

> **Main Task of POS Tagging**: Given a training set, find a function that maps a sentence to its corresponding POS tag sequence.

---

### Formal Definition

**1. 句子与词性标注序列的表示**

- 一个长度为 \( n \) 的句子可以表示为：
  \[
  x_1, x_2, x_3, \dots, x_n
  \]
  其中，\( x_i \) 是句子中的第 \( i \) 个词。

- 对应的词性标注（POS Tagging）序列表示为：
  \[
  y_1, y_2, y_3, \dots, y_n
  \]
  其中，\( y_i \) 是第 \( i \) 个词 \( x_i \) 的词性标签。

---

**2. 联合概率（Joint Probability）**
- 我们关注的是句子与其词性标注序列的**联合概率（Joint Probability）**：
  \[
  p(x_1, x_2, x_3, \dots, x_n, y_1, y_2, y_3, \dots, y_n)
  \]

- 这个联合概率表示句子 \( x_1, x_2, \dots, x_n \) 与词性标注序列 \( y_1, y_2, \dots, y_n \) 同时出现的可能性。

---

**3. 最可能的词性标注序列**

- 目标是找到给定句子 \( x_1, x_2, \dots, x_n \) 的**最可能的词性标注序列（Most Likely POS Tag Sequence）**。

- 这可以形式化为：
  \[
  \arg\max_{y_1, y_2, \dots, y_n} p(y_1, y_2, \dots, y_n \mid x_1, x_2, \dots, x_n)
  \]

- 使用**贝叶斯定理（Bayes' Rule）**，可以将其改写为：
  \[
  \arg\max_{y_1, y_2, \dots, y_n} p(y_1, y_2, \dots, y_n) \cdot p(x_1, x_2, \dots, x_n \mid y_1, y_2, \dots, y_n)
  \]

---

**4. 关键组成部分**
1. **先验概率（Prior Probability）**：
   $$
   p(y_1, y_2, \dots, y_n)
   $$
   - 这表示词性标注序列 \( y_1, y_2, \dots, y_n \) 独立于句子出现的概率。

2. **似然（Likelihood）**：
   $$
   p(x_1, x_2, \dots, x_n \mid y_1, y_2, \dots, y_n)
   $$
   - 这表示在给定词性标注序列 \( y_1, y_2, \dots, y_n \) 的情况下，句子 \( x_1, x_2, \dots, x_n \) 出现的概率。

---

**5. 直观理解**
- **最可能的词性标注序列**是使得以下两项乘积最大化的序列：
  1. 词性标注序列的先验概率 \( p(y_1, y_2, \dots, y_n) \)。
  2. 给定词性标注序列的句子似然 \( p(x_1, x_2, \dots, x_n \mid y_1, y_2, \dots, y_n) \)。

- 这种方法结合了**语言模型（Language Modeling）**（先验概率）和**发射模型（Emission Modeling）**（似然），以找到最佳的词性标注序列。

---

## Solving POS Tag: HMM
我们首先引入**独立性假设**：
$$
p(x_1, x_2, \dots, x_n \mid y_1, y_2, \dots, y_n) = \Pi_{i} p(x_i \mid y_i)
$$

随后我们提出一种模型：Hidden Markov Model (HMM)，中文是“隐马尔可夫模型”。

---

### 模型定义

**1. 基本元素**
- **状态集合（States）**：词性标注（POS Tags）。
- **输出集合（Outputs）**：词汇（Words）。
- **初始状态（Initial State）**：句子的开始。

---

**2. 参数**
1. **转移概率（Transition Probabilities）**：
   - 二元模型（Bigram）：
     \[
     p(y_n \mid y_{n-1})
     \]
   - 三元模型（Trigram）：
     \[
     p(y_n \mid y_{n-2}, y_{n-1})
     \]

2. **发射概率（Emission Probabilities）**：
   \[
   p(x_n \mid y_n)
   \]

![alt text](image-20.png ':size=70%')

下层是可见的，上层是不可见的。

![alt text](image-21.png ':size=60%')

---

### 参数估计
使用 MLE 进行推导，得出：MLE 最大时概率对应的就是出现频率。

### 解码 - Viterbi Algorithm
给定一个新的句子和训练好的模型，如何指出最好的 POS 序列？使用 Viterbi Algorithm，本质上是一个动态规划。

![alt text](image-29.png ':size=70%')

将乘积问题取对数转化为加和问题，就可以用动态规划求最小路径的方法求解了！

![alt text](image-30.png ':size=70%')

伪代码如下：
![alt text](image-31.png ':size=70%')

更多内容推荐阅读 https://web.stanford.edu/~jurafsky/slp3/A.pdf 。

## Pros & Cons of HMM
* 形式简单，参数估计很直接
* 需要很多训练数据
* 需要一个好的解码算法
* 不能学习特征
