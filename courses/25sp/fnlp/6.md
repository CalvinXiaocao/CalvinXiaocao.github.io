# Lecture 6 - Sequence Tagging II - Linear Models and Beyond
Fundamentals of Natural Language Processing, 2025 Spring

?> 本课程笔记整理自Freefizing & 草鱼。

!> **重点内容：** 

我们继续来分析 Sequence Tagging 问题。

## Features
我们可以把 POS Tagging 当成“特征”，然后用学过的 log-linear model 进行处理。

![alt text](image-32.png ':size=70%')

## Feature-based Discriminative Models
我们使用线性分类器。它具有 $\lambda_{f(x, y)}f(x, y)$ 的形式。其中 $\lambda$ 是权重向量。

$$
\text{score}(x, y) = \sum_{i}\lambda_{f_i(x, y)}f_i(x, y)
$$

线性分类器需要找出使得 score 最大的 y。它的学习关键是：如何找到合适的权重 $\lambda$？我们可以用曾经学过的 log-linear 模型，还可以用 **Perceptron**，也可以用 **SVM**。

## Perceptron
### 算法定义
- 输入：
  - 训练集 $(x_k, y_k)$
  - $x_k$ 是数据，$y_k$ 是标签。
- 初始化：$\lambda_i = 0$
- Define：
  - $z=\argmax_{y\in GEN(x)}\sum_i\lambda_{f_i(x, y)}f_i(x, y)$
- Loop：
  - For $q = 1, 2, \cdots, T$, $k = 1, 2, \cdots, n$, 
  - compute $z=\argmax_{y\in GEN(x)}\sum_i\lambda_{f_i(x, y)}f_i(x, y)$
  - if $z_k \neq y_k$, then $\lambda = \lambda + f(x_k, y_k) - f(x_k, z_k)$
- Output：
  - $\lambda$

### 举例
**China**/N **Mobile**/N **is**/V **a**/DT **communication**/N
**giant**/N **in**/P **east**/ADJ **Asia**/N ...

设置的某些特征：
- $f_1(x, y) = 1$ if current word is giant and $y = N$. $\to f_1(x, y) = 1$

- $f_{11}(x, y) = 1$ if current word is giant and $y = ADJ$. $\to f_{11}(x, y) = 0$

训练到其中一步的时候：我们让感知机预测 **giant**，结果现在输出的不是 N，而是 ADJ

然后我们做更新 $\lambda = \lambda + f(x_k, y_k) - f(x_k, z_k)$。无非是加1或者减1。

## Structured Perceptron Tagger
我们不再一个一个词去打分，而是一个句子一个句子更新。

**New loop**:
- For $q = 1, 2, \cdots, T$, each sentence $s \in S$
  - For each word $x \in s, y \in GEN(x)$
    - compute scores $score_y=\argmax_{y\in GEN(x)}\sum_i\lambda_{f_i(x, y)}f_i(x, y)$
  - Now score is a **lattice** $|s| \times |y|$
  - Decode the best sequence for sentence $s$ with the **Viterbi Algorithm**
  - Update $\lambda$s

本质：对一句话进行更新。为什么？因为我们现在可以引入像这样的特征了：

$f_2(x, y) = 1$ if previous word is $S$ and $y = N$.

**Decode the currently best tag sequence for the whole
sentence**

### An Alternative: Beam Search

**输入：**  
- 长度为 **n** 的输入句子 **x**  
- 训练好的模型评分函数 **score(∗)**  
- 预定义的 **束宽（beam size）k**  

**算法流程：**  
1. 用 $H_i$ 存储当前位置 $i \in {1, 2, ..., n}$ 的所有可能候选序列  
2. 初始化 $H_0$（如空序列或起始标记）  
3. 对于每个位置 **i**：  
   - 使用临时存储 **C** 存放新生成的候选序列  
   - 对于 **$H_{i−1}$** 中的每个候选序列 **$y^H_{1:i−1}$**：  
     - 尝试所有可能的标签 **$y^h_i$**，生成新序列 **$y^H_{1:i−1}, y^h_i$**  
     - 计算其得分并存入 **C**  
   - 从 **C** 中选择 **得分最高的 k 个** 序列，构成 **$H_i$**  
4. **输出：** **$H_n$** 中得分最高的序列  

### **特点：**  
- **实现简单**，**通常高效且有效**  
- **不保证全局最优**（可能错过最佳序列）  
- **时间复杂度**：$O(n²k|y|)$（|y| 为可能的标签数）  
- **空间复杂度**：$O(n^2k)$

## Tagging With Global Feature

`todo: local vs global`

`todo: Conditional Random Fields`

## Neural Sequence Tagger
在神经网络下，无需特征工程，可以用 CNN，RNN，LSTM 等

