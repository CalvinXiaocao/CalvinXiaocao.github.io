# Lecture 4 - Language Model
Fundamentals of Natural Language Processing, 2025 Spring

?> 本课程笔记整理自Freefizing & 草鱼。

!> **重点内容：** Language Model的概念; unigram, bigram, trigram; 混淆度（Perplexity）的计算。

给定一串英语单词，我们如何判断这串表达是好的还是坏的?

词汇分析？语法分析？句法分析？

但是我们的期望却是：依赖**数据**进行打分

假设我们有一个词汇表 \(\vartheta\) ，由这些不重复的单词可以随机组合成一个接近无穷的句子集合 \(S\) ，每个句子是 \(s\) ，那么如果想判别每个句子它真正出现在人类语言中的概率，也就是给每个句子打分 \(p\) ，我们希望越“好”的句子分数越高，反之越低。

凭直觉，我们会设计 \(p(s)=\frac{count(s)}{\left| S \right|}\) 其中count是我们在总集合中对 \(s\) 的计数。

但仔细想想漏洞百出：首先我们不知道 \(S\) 里到底有多少句话，其次， \(s\) 可能有多种形式出现（比如包含在别的局句子里），我们根本没有办法穷尽对它们的计数。
换种角度：既然句子是一系列词组成的序列而非集合（set具有无序性），那么我们可以尝试计算 \(P(X_{1}=w_{1},X_{2}=w_{2},...,X_{n}=STOP)\)

好处就是：
1. \(X_1, X_2, \dots, X_n\) 每个地方都需要特定的单词，确定了一个序列而非集合！
2. 有了 STOP 和 START 这种标识符，我们可以框定序列出现的情形：单独出现，作为结尾，作为开头。从而忽略掉不符合我们要求的出现方式。
   
只是还有个问题：根据链式法则，
$$
P(X_{1}=w_{1},X_{2}=w_{2},...,X_{n}=STOP)=P(X_{1}=w_{1})\prod_{i=2}^{n}P(X_{i}=w_{i}|X_{1}=w_{1},...,X_{i-1}=w_{i-1})
$$
当 n 非常大时，这个式子的“历史”部分会变得特别长。其实在很多情况下，是不必要有那么长的历史的（比如“吃得好”，前面可以搭配任何人和地点和时间，其实并不影响这三个字的意思）。

此时，我们需要做一点假设来决定历史的长度。

## N-Gram Model

* 0历史——unigram：First-order Markov assumption，几乎每个词都是独立出现的，不互相影响。$$P(X_{1}=w_{1},X_{2}=w_{2},...,X_{n}=STOP) =P(X_{1}=w_{1})P(X_{2}=w_{2})P(X_3=w_{3}).....P(X_{n}=STOP)$$
* 1历史——bigram：Second-order Markov assumption，每个词只与它前面的1个词有关。
* 2历史——Trigram：3rd-order Markov assumption，每个词只与它前面的2个词有关。

![alt text](image-5.png 'size=60%')

![alt text](image-6.png 'size=60%')

注意：p(the) 可以看成是 p(the|start,start)，p(cat|the) 可以看成是 p(cat|the,start)

下面要做的是估计参数量：

要计算的参数核心：在 bigram 里，\(p(w_i|w_{i-1})\) ；trigram 里，\(p(w_i|w_{i-1},w_{i-2})\) 。

$$
p(w_i|w_{i-1})=\frac{count(w_{i-1},w_i)}{count(w_{i-1})}, p(w_i|w_{i-1},w_{i-2})=\frac{count(w_{i-2},w_{i-1},w_i)}{count(w_{i-2},w_{i-1})}
$$

所以我们的模型究竟有多大呢？

![alt text](image-7.png 'size=60%')

图中的words是篇章中的单词数，并不是词汇表大小。可以看出，unigram的参数量就是词汇表的大小716706，那为什么bigram LM不是预期中的716706的平方值之多呢？因为稀疏度。只有1/4000000左右的参数非零。

## 模型的评估 - Perplexity

语言模型就是个建模自然语言的概率模型，它的任务是评估一个给定的词序列(即一个句子)在真实世界中出现的概率。所以语言模型不单单是用于文本生成，还可以用来做语言评估之类的。

接下来的问题是，我们如何对我们模型的效果进行评估呢？

已知测试语料集 \(S\)，我们的语言模型做的工作是给每个序列出现的可能性打分 \(p(s)=p(w_1,w_{2},...w_n)\) , s是 \(S\)中一个序列，n是s中的单词数，w是具体的单词。

设 \(S\) 中一共有M个单词（ \(\neq\) 词汇表大小），那么对整个数据集来说，我们想知道一个序列token在 \(S\) 中出现的所有地方的打分的积 \(\prod_{s\epsilon S}^{}p(s)\) 作为模型会生成这个token的概率打分，对于总词数 $M$ 进行 normalize 去掉语料长度的影响，再进行 $\log$，将概率值转化为一种“惩罚”，对数函数将大于1的值映射到正数，将小于1的值映射到负数，可以得到 \(\frac{1}{M}\sum_{s\epsilon S}^{}{\log p(s)}\) , 加个负号，再放在指数上进行放大，就可以得到指标——混淆度计算公式。

**混淆度（歧义度）概念：** $$Perplexity(S)=2^{-\frac{1}{M}\sum_{s\epsilon S}{\log p(s)}}$$

衡量一个语言模型在未见过的的语料 \(S\) 上的表现，取值范围 \((1, +\inf)\)，这个句子越好，里面的s序列的打分$p(s)$当然就越高 $\to$1，$\log p(s)$ 就越接近0，指数越接近0，混淆度越低，接近1。

看一个例子：
![alt text](image-8.png)

**TODO**

## Nerual Language Models
一个基本的单元：
$$
z = \sum_i w_i x_i + b
$$

* $x_i$ 是输入的特征
* $w_i$ 是权重
* $b$ 是偏置
* $z$ 是输出
* 这是一个线性模型

同时我们在线性模型中加入非线性成分

* Sigmoid $$y = \frac{1}{1+e^{-z}}$$
* Tanh $$y = \frac{e^z-e^{-z}}{e^z+e^{-z}}$$
* ReLU $$y = max(0,z)$$

现在我们可以基于这些小 block 搭建我们的神经网络了。

### NLM Version 1
linear, non-linear, linear, non-linear

$$
p(w_i|w_{i-1},w_{i-2},...,w_1) = softmax(b_{out} + \sum_{j=1}^{n-1}w_jA_{j} + W \tanh (u + \sum_{j=1}^{n-1}w_jT_{j}))
$$

* parameters: $b, A, W, T, u$
* vocabulary size: $V$, hidden size: $H$

> $w_j$ 是第j个词的bag-of-words 或者 one-hot 表示。可是 $w_j$ 太稀疏了，我们可以用embedding来表示。具体而言，找到一个 $M$ of $(V, d)$, 使得 $w_i^T M = m_i$，其中 $m_i$ 是 $w_i$ 的 embedding, 长度为 $d$。

$$
p(w_i|w_{i-1},w_{i-2},...,w_1) = softmax(b_{out} + \sum_{j=1}^{n-1} m_jA_{j} + W \tanh (u + \sum_{j=1}^{n-1}m_jT_{j}))
$$

$b(V ), A(d, V), W(V,H), T(d,H), u(H), M(V, d)$

?> 这是 2003 年提出的模型 [Bengio et al., 2003]。可为什么当时没有那么多人关注呢？因为它太慢了……计算量实在大，参数量太多了。可是再仔细想想…… $\sum_{j=1}^{n-1}w_jA_{j}$ 到底有必要吗？尽管可能有一点点效果的提升，但是这个提升是不是值得这么大的计算量呢？


