# Lecture 2 - Word Sense Disambiguation
Fundamentals of Natural Language Processing, 2025 Spring

?> 本课程笔记整理自草鱼，部分补充内容由 deepseek 或 gpt 生成。如有错误，欢迎指正！

## Background
词义消歧（WSD）是自然语言处理（NLP）中的一项核心任务，旨在根据上下文确定多义词在特定语境中的正确含义。例如，“bank”一词既可指“银行”，也可指“河岸”，WSD的任务就是通过上下文判断其具体含义。

在自然语言处理中，词义的多义性是一个常见且重要的挑战。一个词可能具有多个不同的含义，这些含义之间的差异可能体现在多个层次上。例如，**同形异义词（homonymy）**指的是拼写或发音相同但意义完全不同的词，比如“bank”既可以指“银行”也可以指“河岸”，这两个意义之间没有任何关联。另一方面，**多义词（polysemy）**则是指一个词有多个相关但不同的意义，比如“head”可以指“头部”也可以指“领导”，这些意义之间存在一定的联系。然而，同形异义词和多义词之间的界限并不总是清晰的。很多情况下，我们需要指出具体的词汇含义。同时有的词使用时还会出现“双关”。

## Definition
现在让我们给出WSD问题的一般性定义。

?> **Definition: World Sense Disambiguation problem**
> 给定一个多义词及其特定的上下文，我们需要确定目标词在该上下文中被激活的是哪个意义。
> * **Input:** 目标词及其上下文，以及一组可能的词义
> * **Model:** 提取各种特征并与已有知识进行比较
> * **Output:** 目标词在该上下文中的词义标签

## WordNet
在开始前，计算机需要知道关于某个词汇的全部含义。这里WordNet就派上用场了。

**WordNet简介**  
WordNet是由Princeton大学开发的联机英语词汇检索系统，与传统按字母顺序组织的词典不同，它从语义角度组织词汇信息，是一部语义词典（Semantic Lexicon）和语言学本体库（Linguistic Ontology）。WordNet将词汇分为名词、动词、形容词和副词四大类，忽略虚词，其核心特色是根据词义而非词形组织词汇，采用同义词集合（Synonymy Set）作为基本单元，体现词形间的语义关系。例如，WordNet 2.0版本包含115,424个同义词集合，其中名词占79,685个，覆盖了常用英语名词词汇。这种独特的组织结构使WordNet在自然语言处理研究中具有广泛应用价值。

## Approaches
解决WSD问题的方法有很多，可以分成如下三类。

**1. 监督学习（Supervised Learning）**
  
- **任务类型**：分类任务（classification task）。  
- **特点**：  
  - 已知词义清单（sense inventories are known）。  
  - 有标注数据可用于训练（annotated data available for training）。  
- **适用场景**：当词义定义明确且标注数据充足时，监督学习方法能够直接学习词义与上下文之间的映射关系。  

**2. 无监督学习（Unsupervised Learning）**  

- **任务类型**：聚类任务（clustering task）。  
- **特点**：  
  - 词义清单未知或不完整（we do not know/have the complete sense inventories）。  
  - 仅有纯文本数据可用（plain text data available only）。  
  - 将数据实例聚类成组：组内相似，组间区分（cluster data instances into groups: similar within a group, discriminative between groups）。  
- **适用场景**：当缺乏标注数据时，无监督学习方法通过聚类发现词义的潜在模式。

?> 💡 **什么是聚类？** **聚类（Clustering）**是一种无监督学习方法，旨在将数据集中的对象**分组**，使得同一组（簇）内的对象彼此相似，而不同组之间的对象差异较大。聚类通过分析数据的特征或属性，自动发现数据中的潜在结构或模式，而无需预先标注类别信息。常见的聚类算法包括K均值（K-means）、层次聚类（Hierarchical Clustering）和DBSCAN等。聚类广泛应用于数据挖掘、图像分割、市场细分和自然语言处理等领域。

**3. 半监督学习（Semi-Supervised Learning）**

- **特点**：  
  - 仅有少量标注数据（limited data with annotations）。  
  - 通过间接监督（如WordNet中的词条或例句）进行学习（indirect supervision: entries or example sentences in WordNet）。  
  - 可利用大量文本资源（a lot more textual resources）。  
- **适用场景**：在标注数据有限但未标注数据丰富的情况下，半监督学习方法结合少量标注数据和大量未标注数据，提升模型性能。  
 
词义消歧任务可根据数据条件选择不同的学习方法：监督学习依赖标注数据，无监督学习通过聚类发现模式，而半监督学习则结合少量标注数据和大量未标注数据，在资源有限的情况下实现更好的性能。下面依次对这三种方法进行详细介绍。

### Supervised Learning

**主要步骤**

1. **获取训练数据**：收集带有标注词义标签的数据实例（data instances with annotated sense labels）。  
2. **特征提取**：为每个实例提取多种特征（extract various features for each instance）。  
3. **模型训练**：通过数学模型学习带有特定标签的实例的特征模式（learn what an instance with label X should look like）。  
4. **预测词义**：对未见过的实例提取特征，并利用模型预测最合适的词义标签（predict which label is the most suitable one）。  

**优缺点**

- **优点**：  
  - 方法多样，可选择范围广（a wide range of choices）。  
  - 性能优越，具有竞争力（competitive performances）。  
- **缺点**：  
  - 需要大量标注数据作为训练集（need annotated instances as training data）。  
  - 数据质量对模型性能影响较大（data quality is crucial）。  

### Semi-supervised Learning

**核心思想**

从少量标注数据（种子）出发，通过自举（bootstrapping）方法逐步扩展训练集。  

**主要步骤**

1. **获取种子数据**：准备一个小规模的手工标注语料库作为初始种子（small manually annotated corpus as seeds）。  
2. **自举扩展标注数据**：  
   - 利用现有分类器的Top K输出结果（top K output from existing classifiers）。  
   - 设计规则从自然对齐数据集中提取更多样本（design rules to extract more samples, e.g., from naturally aligned datasets）。  
   - 示例：  
     - “I should go to the bank and discuss with the clerks.” → 银行  
     - “I enjoy fishing at the bank.” → 河边  
3. **迭代过程**：通常需要重复上述步骤以扩展数据集。  
4. **监督学习模式**：最终采用监督学习的方法训练模型。  

**优缺点**

- **优点**：  
  - 易于获得更大规模的训练数据（easy to obtain a larger set of training data）。  
- **缺点**：  
  - 数据质量难以评估（hard to decide/evaluate the quality of the data）。  
  - 实际应用中需要许多技巧才能取得良好效果（require many tricks to make it work in practice）。  

### Unsupervised Learning

**核心思想**

相似的词义往往出现在相似的上下文中（similar senses occur in similar context）。  

**主要步骤**

1. **特征提取**：从上下文中提取特征（extract features from context）。  
2. **聚类**：根据特征对词的用法进行分组（group word mentions/usages accordingly）。  
3. **表示词义**：对聚类结果进行合理的词义表示（properly represent the induced senses）。  

**优缺点**

- **优点**：  
  - 无需收集标注数据（no need to collect training data）。  
- **缺点**：  
  - 聚类结果难以解释（hard to interpret the clustering results）。  
  - 仍需人工干预以应用于实际任务，例如将聚类结果映射到现有词义清单（still require human involvement for future applications, e.g., mapping the clusters to an existing sense inventory）。

## Let's start solving it!

现在让我们着手尝试解决WSD问题。我们选取Supervised learning 方式。

**1. 上下文特征提取**  

对于目标词及其上下文，提取以下特征：  
- **邻近词**（neighboring words）：目标词周围的词语。  
- **邻近N元组和局部搭配**（neighboring N-grams, local collocations）：目标词附近的短语或固定搭配。  
- **目标词的词性标注**（Part-of-speech (POS) tags of the target word）：目标词的词性信息。  
- **邻近词的词性标注**（POS tags of the neighboring words）：目标词周围词语的词性信息。  
- **分布语义模型**（distributional semantic models）：如潜在语义分析（LSA）、潜在狄利克雷分布（LDA）、神经网络（NN）等，用于捕捉词语的语义分布。  

**2. 模型选择**  

选择合适的数学模型进行训练和预测，常用的模型包括：  
- **朴素贝叶斯**（Naïve Bayes）：基于概率的分类模型。  
- **线性模型**（Linear models）：如线性回归。  
- **逻辑回归**（Logistic Regression）：用于二分类或多分类任务。  
- **支持向量机**（Support Vector Machines, SVM）和**最大熵模型**（Maximum Entropy）：适用于高维特征空间的分类。  
- **神经网络模型**（Neural Network Models）：如多层感知机（MLP）或深度学习模型，能够捕捉复杂的非线性关系。  

这里我们选择使用**临近词**和**朴素贝叶斯**方法进行训练。

## Naïve Bayes for WSD
我们从最简单的朴素贝叶斯方法开始。

?> Recall: **Bayes' Theorem**
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

### 主要步骤

1. **训练阶段**：  
   - 基于训练数据，估计以下概率：  
     - \( P(A) \)：类别 \( A \) 的先验概率。  
     - \( P(B) \)：特征 \( B \) 的概率。  
     - \( P(B|A) \)：在类别 \( A \) 下特征 \( B \) 的条件概率。  

2. **推理阶段**：  
   - 对测试数据进行推断，找到最合适的类别：  
     - 通过贝叶斯定理计算后验概率 \( P(A|B) \)，即给定特征 \( B \) 时类别 \( A \) 的概率。  
     - 选择使 \( P(A|B) \) 最大化的类别作为预测结果。  

### 定义概率模型

在词义消歧任务中，我们通过概率模型来确定目标词 \( w \) 在上下文 \( C \) 中的最可能词义 \( s^* \)。具体步骤如下：

**定义符号**

- \( w \)：目标词（the target word）。  
- \( S = \{s_1, s_2, \dots, s_n\} \)：目标词 \( w \) 的词义清单（the sense inventory for \( w \)）。  
- \( C \)：目标词 \( w \) 的上下文（the context of \( w \)）。  
- \( V = \{v_1, v_2, \dots, v_j\} \)：上下文特征词汇表（the vocabulary of context features）。  

**定义目标**

为目标词 \( w \) 在上下文 \( C \) 中分配最可能的词义 \( s^* \)，即：  
$$
s^* = \arg\max_{s_k} P(s_k | C)
$$
其中，\( P(s_k | C) \) 表示在上下文 \( C \) 下词义 \( s_k \) 的后验概率。  

**使用贝叶斯定理**

根据贝叶斯定理，后验概率 \( P(s_k | C) \) 可以表示为：  

$$P(s_k | C) \propto P(C | s_k) \cdot P(s_k)$$
 
因此，最优词义 \( s^* \) 可以通过最大化以下公式得到：  

$$s^* = \arg\max_{s_k} P(C | s_k) \cdot P(s_k)$$
 
其中：  
- \( P(s_k) \)：词义 \( s_k \) 的先验概率。  
- \( P(C | s_k) \)：在词义 \( s_k \) 下上下文 \( C \) 的似然概率。  

为了估计 \( P(C|s_k) \) 和 \( P(s_k) \)，朴素贝叶斯方法引入了**独立性假设**，尽管这一假设在现实中并不完全成立，但它大大简化了模型的计算。

**独立性假设**

假设上下文 \( C \) 中的每个特征 \( v_x \) 在给定词义 \( s_k \) 的条件下是相互独立的，即：  

$$P(C|s_k) = P(\{v_x | v_x \in C\}|s_k) = \prod_{v_x \in C} P(v_x|s_k)$$

这意味着上下文中的每个词或特征对词义的影响是独立的，一个词的出现不会影响其他词的出现概率。  

### 概率估计与测试过程

在朴素贝叶斯框架下，词义消歧任务涉及对条件概率 \( P(v_x|s_k) \) 和先验概率 \( P(s_k) \) 的估计，以及对未见实例的测试过程。以下是具体步骤：

**1. 概率估计**

- **条件概率 \( P(v_x|s_k) \)**：  
  表示在词义 \( s_k \) 下，特征 \( v_x \) 出现的概率。计算公式为：  
  $$
  P(v_x|s_k) = \frac{\text{Count}(v_x, s_k)}{\text{Count}(s_k)}
  $$ 
  其中：  
  - \(\text{Count}(v_x, s_k)\)：特征 \( v_x \) 在词义 \( s_k \) 下的出现次数。  
  - \(\text{Count}(s_k)\)：词义 \( s_k \) 在训练数据中的总出现次数。  

- **先验概率 \( P(s_k) \)**：  
  表示词义 \( s_k \) 的先验概率。计算公式为：  
  $$
  P(s_k) = \frac{\text{Count}(s_k)}{\text{Count}(w)}
  $$ 
  其中：  
  - \(\text{Count}(s_k)\)：词义 \( s_k \) 的出现次数。  
  - \(\text{Count}(w)\)：目标词 \( w \) 在训练数据中的总出现次数。  

**2. 测试过程**  
对于未见实例的上下文 \( C' \)，计算其最可能的词义 \( s^* \)：  
1. **计算后验概率**：  
   对于每个词义 \( s_k \)，计算其后验概率 \( P(s_k|C') \)：  
   $$
   P(s_k|C') \propto P(C'|s_k) \cdot P(s_k) = \prod_{v_x \in C'} P(v_x|s_k) \cdot P(s_k)
   $$  
   其中：  
   - \( P(C'|s_k) \) 通过独立性假设分解为 \( \prod_{v_x \in C'} P(v_x|s_k) \)。  
   - \( P(s_k) \) 是先验概率。  

2. **选择最优词义**：  
   选择使后验概率最大的词义 \( s^* \)：  
   \[
   s^* = \arg\max_{s_k} \prod_{v_x \in C'} P(v_x|s_k) \cdot P(s_k)
   \]  

### 总结
- **训练阶段**：通过统计方法估计条件概率 \( P(v_x|s_k) \) 和先验概率 \( P(s_k) \)。  
- **测试阶段**：利用独立性假设计算未见实例的后验概率，并选择最优词义。  
- **优点**：计算简单高效，适合大规模数据。  
- **局限性**：独立性假设可能不符合实际语言现象，影响模型性能。  

通过上述步骤，朴素贝叶斯方法能够有效地完成词义消歧任务。
