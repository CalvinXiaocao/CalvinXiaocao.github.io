# Lecture 2 - Word Sense Disambiguation
Fundamentals of Natural Language Processing, 2025 Spring

?> 本课程笔记整理自草鱼，部分补充内容由 deepseek 或 gpt 生成。如有错误，欢迎指正！

!> **重点内容：**词义消歧的问题定义；监督学习、无监督学习和半监督学习方法；朴素贝叶斯方法及计算过程；模型的评估（混淆矩阵, Accuracy, Precision, Recall, F值, Micro F1, Macro F1）。

![alt text](image-1.png ':size=60%')

*Source: https://botpenguin.com/glossary/word-sense-disambiguation*

------

## Background
词义消歧（WSD）是自然语言处理（NLP）中的一项核心任务，旨在根据上下文确定多义词在特定语境中的正确含义。例如，“bank”一词既可指“银行”，也可指“河岸”，WSD的任务就是通过上下文判断其具体含义。

在自然语言处理中，词义的多义性是一个常见且重要的挑战。一个词可能具有多个不同的含义，这些含义之间的差异可能体现在多个层次上。例如，**同形异义词（homonymy）**指的是拼写或发音相同但意义完全不同的词，比如“bank”既可以指“银行”也可以指“河岸”，这两个意义之间没有任何关联。另一方面，**多义词（polysemy）**则是指一个词有多个相关但不同的意义，比如“head”可以指“头部”也可以指“领导”，这些意义之间存在一定的联系。然而，同形异义词和多义词之间的界限并不总是清晰的。很多情况下，我们需要指出具体的词汇含义。同时有的词使用时还会出现“双关”。

-----

## Definition
现在让我们给出 **WSD** 问题的一般性定义。

?> **Definition: Word Sense Disambiguation problem**
> 给定一个多义词及其特定的上下文，我们需要确定目标词在该上下文中被激活的是哪个意义。
> * **Input:** 目标词及其上下文，以及一组可能的词义
> * **Model:** 提取各种特征并与已有知识进行比较
> * **Output:** 目标词在该上下文中的词义标签

-----

## WordNet
在开始前，计算机需要知道关于某个词汇的全部含义。这里 WordNet 就派上用场了。

**WordNet 简介**  
WordNet 是由 Princeton 大学开发的联机英语词汇检索系统，与传统按字母顺序组织的词典不同，它从语义角度组织词汇信息，是一部语义词典（Semantic Lexicon）和语言学本体库（Linguistic Ontology）。WordNet将词汇分为名词、动词、形容词和副词四大类，忽略虚词，其核心特色是根据词义而非词形组织词汇，采用同义词集合（Synonymy Set）作为基本单元，体现词形间的语义关系。例如，WordNet 2.0 版本包含 115,424 个同义词集合，其中名词占 79,685 个，覆盖了常用英语名词词汇。这种独特的组织结构使 WordNet 在自然语言处理研究中具有广泛应用价值。

![alt text](image-2.png ':size=60%')

-----

## Approaches
解决 WSD 问题的方法有很多，可以分成如下三类。

**1. 监督学习（Supervised Learning）**
  
- **任务类型**：分类任务（classification task）。  
- **特点**：  
  - 已知词义清单（sense inventories are known）。  
  - 有标注数据可用于训练（annotated data available for training）。
- **适用场景**：当词义定义明确且标注数据充足时，监督学习方法能够直接学习词义与上下文之间的映射关系。  

**2. 无监督学习（Unsupervised Learning）**  

- **任务类型**：聚类任务（clustering task）。  
- **特点**：  
  - 词义清单未知或不完整（we do not know/have the complete sense inventories）。  
  - 仅有纯文本数据可用（plain text data available only）。  
  - 将数据实例聚类成组：组内相似，组间区分（cluster data instances: similar within a group, discriminative between groups）。
- **适用场景**：当缺乏标注数据时，无监督学习方法通过聚类发现词义的潜在模式。

?> 💡 **什么是聚类？** **聚类（Clustering）**是一种无监督学习方法，旨在将数据集中的对象**分组**，使得同一组（簇）内的对象彼此相似，而不同组之间的对象差异较大。聚类通过分析数据的特征或属性，自动发现数据中的潜在结构或模式，而无需预先标注类别信息。常见的聚类算法包括K均值（K-means）、层次聚类（Hierarchical Clustering）和DBSCAN等。聚类广泛应用于数据挖掘、图像分割、市场细分和自然语言处理等领域。

**3. 半监督学习（Semi-Supervised Learning）**

- **特点**：  
  - 仅有少量标注数据（limited data with annotations）。  
  - 通过间接监督（如WordNet中的词条或例句）进行学习（indirect supervision: entries or example sentences in WordNet）。  
  - 可利用大量文本资源（a lot more textual resources）。
- **适用场景**：在标注数据有限但未标注数据丰富的情况下，半监督学习方法结合少量标注数据和大量未标注数据，提升模型性能。  
 
词义消歧任务可根据数据条件选择不同的学习方法：监督学习依赖标注数据，无监督学习通过聚类发现模式，而半监督学习则结合少量标注数据和大量未标注数据，在资源有限的情况下实现更好的性能。下面依次对这三种方法进行详细介绍。

-----

### Supervised Learning

**主要步骤**

1. **获取训练数据**：收集带有标注词义标签的数据实例（data instances with annotated sense labels）。  
2. **特征提取**：为每个实例提取多种特征（extract various features for each instance）。  
3. **模型训练**：通过数学模型学习带有特定标签的实例的特征模式（learn what an instance with label X should look like）。  
4. **预测词义**：对未见过的实例提取特征，并利用模型预测最合适的词义标签（predict which label is the most suitable one）。  

**优缺点**

- **优点**：  
  - 方法多样，可选择范围广（a wide range of choices）。  
  - 性能优越，具有竞争力（competitive performances）。  
- **缺点**：  
  - 需要大量标注数据作为训练集（need annotated instances as training data）。  
  - 数据质量对模型性能影响较大（data quality is crucial）。  

-----

### Semi-supervised Learning

**核心思想**

从少量标注数据（种子）出发，通过自举（bootstrapping）方法逐步扩展训练集。  

**主要步骤**

1. **获取种子数据**：准备一个小规模的手工标注语料库作为初始种子（small manually annotated corpus as seeds）。  
2. **自举扩展标注数据**：  
   - 利用现有分类器的Top K输出结果（top K output from existing classifiers）。  
   - 设计规则从自然对齐数据集中提取更多样本（design rules to extract more samples, e.g., from naturally aligned datasets）。  
   - 示例：  
     - “I should go to the bank and discuss with the clerks.” → 银行  
     - “I enjoy fishing at the bank.” → 河边  
3. **迭代过程**：通常需要重复上述步骤以扩展数据集。  
4. **监督学习模式**：最终采用监督学习的方法训练模型。  

**优缺点**

- **优点**：  
  - 易于获得更大规模的训练数据（easy to obtain a larger set of training data）。  
- **缺点**：  
  - 数据质量难以评估（hard to decide/evaluate the quality of the data）。  
  - 实际应用中需要许多技巧才能取得良好效果（require many tricks to make it work in practice）。  

-----

### Unsupervised Learning

**核心思想**

相似的词义往往出现在相似的上下文中（similar senses occur in similar context）。  

**主要步骤**

1. **特征提取**：从上下文中提取特征（extract features from context）。  
2. **聚类**：根据特征对词的用法进行分组（group word mentions/usages accordingly）。  
3. **表示词义**：对聚类结果进行合理的词义表示（properly represent the induced senses）。  

**优缺点**

- **优点**：  
  - 无需收集标注数据（no need to collect training data）。  
- **缺点**：  
  - 聚类结果难以解释（hard to interpret the clustering results）。  
  - 仍需人工干预以应用于实际任务，例如将聚类结果映射到现有词义清单（still require human involvement for future applications, e.g., mapping the clusters to an existing sense inventory）。

![alt text](image-3.png ':size=60%')

-----

## Let's start solving it!

现在让我们着手尝试解决WSD问题。我们选取 Supervised learning 方式。关键步骤如下两步。

**1. 上下文特征提取**  

对于目标词及其上下文，提取以下特征：  
- **邻近词**（neighboring words）：目标词周围的词语。  
- **邻近N元组和局部搭配**（neighboring N-grams, local collocations）：目标词附近的短语或固定搭配。  
- **目标词的词性标注**（Part-of-speech (POS) tags of the target word）：目标词的词性信息。  
- **邻近词的词性标注**（POS tags of the neighboring words）：目标词周围词语的词性信息。  
- **分布语义模型**（distributional semantic models）：如潜在语义分析（LSA）、潜在狄利克雷分布（LDA）、神经网络（NN）等，用于捕捉词语的语义分布。  

**2. 模型选择**  

选择合适的数学模型进行训练和预测，常用的模型包括：  
- **朴素贝叶斯**（Naïve Bayes）：基于概率的分类模型。  
- **线性模型**（Linear models）：如线性回归。  
- **逻辑回归**（Logistic Regression）：用于二分类或多分类任务。  
- **支持向量机**（Support Vector Machines, SVM）和**最大熵模型**（Maximum Entropy）：适用于高维特征空间的分类。  
- **神经网络模型**（Neural Network Models）：如多层感知机（MLP）或深度学习模型，能够捕捉复杂的非线性关系。  

这里我们选择使用**临近词**和**朴素贝叶斯**方法进行训练。

-----

## Naïve Bayes for WSD
我们从最简单的朴素贝叶斯方法开始。

?> Recall: **Bayes' Theorem**
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

### 主要步骤

1. **训练阶段**：  
   - 基于训练数据，估计以下概率：  
     - \( P(A) \)：类别 \( A \) 的先验概率。  
     - \( P(B) \)：特征 \( B \) 的概率。  
     - \( P(B|A) \)：在类别 \( A \) 下特征 \( B \) 的条件概率。  

2. **推理阶段**：  
   - 对测试数据进行推断，找到最合适的类别：  
     - 通过贝叶斯定理计算后验概率 \( P(A|B) \)，即给定特征 \( B \) 时类别 \( A \) 的概率。  
     - 选择使 \( P(A|B) \) 最大化的类别作为预测结果。  

-----

!> **计算 Bayes 模型是重点！**

?> 例：我们现在对 **bank** 这个词做词义消歧。**bank** 有两种含义：1. 金融机构，2. 河岸。训练集有四个句子如下。下面我们来用它建立模型。

| 句子（上下文 \( C \)）                                     | 词义 \( s_k \)   |
|-----------------------------------------------------------|------------------|
| "deposit money bank cash deposit"                         | \( s_1 \)        |
| "river bank water flows river"                             | \( s_2 \)        |
| "bank loan interest bank"                                  | \( s_1 \)        |
| "fish bank river sand"                                     | \( s_2 \)        |

测试语句：river bank interest deposit

-----

### 定义概率模型

在词义消歧任务中，我们通过概率模型来确定目标词 \( w \) 在上下文 \( C \) 中的最可能词义 \( s^* \)。具体步骤如下：

**定义符号**

- \( w \)：目标词（the target word）。  
- \( S = \{s_1, s_2, \dots, s_n\} \)：目标词 \( w \) 的词义清单（the sense inventory for \( w \)）。  
- \( C \)：目标词 \( w \) 的上下文（the context of \( w \)）。  
- \( V = \{v_1, v_2, \dots, v_j\} \)：上下文特征词汇表（the vocabulary of context features）。  

?> 分别指出本例中的几个符号吧！$w=$ bank，$S = \{s_1, s_2\}$，$C$ 是句子上下文。

**定义目标**

为目标词 \( w \) 在上下文 \( C \) 中分配最可能的词义 \( s^* \)，即：  
$$
s^* = \arg\max_{s_k} P(s_k | C)
$$
其中，\( P(s_k | C) \) 表示在上下文 \( C \) 下词义 \( s_k \) 的后验概率。  

?> 本例中，\( C' = \{\text{river}:1, \text{bank}:1, \text{interest}:1, \text{deposit}:1\} \)。我们需要比较 $s_1$ 和 $s_2$，计算在给定 C 的条件下，谁发生的概率最大。

**使用贝叶斯定理**

根据贝叶斯定理，后验概率 \( P(s_k | C) \) 可以表示为：  

$$P(s_k | C) \propto P(C | s_k) \cdot P(s_k)$$

`注意这里我们省略了分母部分，因为不同 k 对应的 P(C) 值是相同的，我们不必计算。`
 
因此，最优词义 \( s^* \) 可以通过最大化以下公式得到：  

$$s^* = \arg\max_{s_k} P(C | s_k) \cdot P(s_k)$$
 
其中：  
- \( P(s_k) \)：词义 \( s_k \) 的先验概率。  
- \( P(C | s_k) \)：在词义 \( s_k \) 下上下文 \( C \) 的似然概率。  

为了估计 \( P(C|s_k) \) 和 \( P(s_k) \)，朴素贝叶斯方法引入了**独立性假设**，尽管这一假设在现实中并不完全成立，但它大大简化了模型的计算。

**独立性假设**

假设上下文 \( C \) 中的每个特征 \( v_x \) 在给定词义 \( s_k \) 的条件下是相互独立的，即：  

$$P(C|s_k) = P(\{v_x | v_x \in C\}|s_k) = \prod_{v_x \in C} P(v_x|s_k)$$

这意味着上下文中的每个词或特征对词义的影响是独立的，一个词的出现不会影响其他词的出现概率。  

-----

### 概率估计与测试过程

在朴素贝叶斯框架下，词义消歧任务涉及对条件概率 \( P(v_x|s_k) \) 和先验概率 \( P(s_k) \) 的估计，以及对未见实例的测试过程。以下是具体步骤：

**1. 概率估计**

- **条件概率** \( P(v_x|s_k) \)：  
  表示在词义 \( s_k \) 下，特征 \( v_x \) 出现的概率。计算公式为：  
  $$
  P(v_x|s_k) = \frac{\text{Count}(v_x, s_k)}{\text{Count}(s_k)}
  $$ 
  其中：  
  - \(\text{Count}(v_x, s_k)\)：特征 \( v_x \) 在词义 \( s_k \) 下的出现次数。  
  - \(\text{Count}(s_k)\)：词义为 \( s_k \) 的句子的个数。

在本例中，我们首先对每个词出现的频率进行统计。

| 词义 \( s_k \) | 句子ID | 词频（Bag-of-Words）                          |
|----------------|--------|-----------------------------------------------|
| \( s_1 \)      | 1      | {"deposit":2, "money":1, "bank":1, "cash":1} |
| \( s_1 \)      | 3      | {"bank":2, "loan":1, "interest":1}           |
| \( s_2 \)      | 2      | {"river":2, "bank":1, "water":1, "flows":1}   |
| \( s_2 \)      | 4      | {"fish":1, "bank":1, "river":1, "sand":1}    |

接着我们来算条件概率。统计每个词义下所有句子的词频总和（注意：同一个词在不同句子中的频次累加）：

  - **\( s_1 \) 的词频统计**：
    - "deposit":2, "money":1, "bank":3（句子1中1次 + 句子3中2次）, "cash":1, "loan":1, "interest":1  
    - 总词数 \( \text{Count}(s_1) = 2+1+3+1+1+1 = 9 \)

  - **\( s_2 \) 的词频统计**：
    - "river":3（句子2中2次 + 句子4中1次）, "bank":2, "water":1, "flows":1, "fish":1, "sand":1  
    - 总词数 \( \text{Count}(s_2) = 3+2+1+1+1+1 = 9 \)

  - 计算条件概率（使用拉普拉斯平滑，\( \alpha=1 \)，词汇表大小 \( |V|=10 \)）：
    - 例如：  
      \( P(\text{river} \| s_1) = \frac{0 + 1}{9 + 10} = \frac{1}{19} \)（"river" 未在 \( s_1 \) 中出现过）  
      \( P(\text{bank} \| s_1) = \frac{3 + 1}{9 + 10} = \frac{4}{19} \)  
      \( P(\text{river} \| s_2) = \frac{3 + 1}{9 + 10} = \frac{4}{19} \)

?> 拉普拉斯平滑：如果直接去按 count 计算，可能会出现 count = 0 的情况，导致概率为 0。我们引入 laplacian 平滑：$$P(v_x|s_k) = \frac{\text{Count}(v_x, s_k) + \alpha}{\text{Count}(s_k) + \alpha |V|}$$这可以有效解决概率为 0 的问题。

---



- **先验概率** \( P(s_k) \)：  
  表示词义 \( s_k \) 的先验概率。计算公式为：  
  $$
  P(s_k) = \frac{\text{Count}(s_k)}{\text{Count}(w)}
  $$ 
  其中：  
  - \(\text{Count}(s_k)\)：词义 \( s_k \) 的出现次数。  
  - \(\text{Count}(w)\)：目标词 \( w \) 在训练数据中的总出现次数。 

在本例中，计算先验概率：
  - \( \text{Count}(s_1) = 2 \)（句子1和3）
  - \( \text{Count}(s_2) = 2 \)（句子2和4）
  - \( P(s_1) = P(s_2) = \frac{2}{4} = 0.5 \)。

--- 

**2. 测试过程**  

首先对测试句子提取特征。在本例中，测试句子：

**"river bank interest deposit"**  

提取 Bag-of-Words 特征：\( C' = \{\text{river}:1, \text{bank}:1, \text{interest}:1, \text{deposit}:1\} \)


对于未见实例的上下文 \( C' \)，计算其最可能的词义 \( s^* \)：  
1. **计算后验概率**：  
   对于每个词义 \( s_k \)，计算其后验概率 \( P(s_k|C') \)：  
   $$
   P(s_k|C') \propto P(C'|s_k) \cdot P(s_k) = \prod_{v_x \in C'} P(v_x|s_k) \cdot P(s_k)
   $$  
   其中：  
   - \( P(C'|s_k) \) 通过独立性假设分解为 \( \prod_{v_x \in C'} P(v_x|s_k) \)。  
   - \( P(s_k) \) 是先验概率。

本例中。对于 \( s_1 \)：
   $$
   P(s_1 \| C') \propto P(s_1) \cdot \prod_{v_x \in C'} P(v_x \| s_1)  
   = 0.5 \cdot P(\text{river} \| s_1) \cdot P(\text{bank} \| s_1) \cdot P(\text{interest} \| s_1) \cdot P(\text{deposit} \| s_1)  
   = 0.5 \cdot \frac{1}{19} \cdot \frac{4}{19} \cdot \frac{2}{19} \cdot \frac{3}{19}  
   \approx 0.5 \cdot 0.00035 \approx 0.000175
   $$
   对于 \( s_2 \)：
  $$
   P(s_2 \| C') \propto P(s_2) \cdot \prod_{v_x \in C'} P(v_x \| s_2)  
   = 0.5 \cdot P(\text{river} \| s_2) \cdot P(\text{bank} \| s_2) \cdot P(\text{interest} \| s_2) \cdot P(\text{deposit} \| s_2)  
   = 0.5 \cdot \frac{4}{19} \cdot \frac{3}{19} \cdot \frac{1}{19} \cdot \frac{1}{19}  
   \approx 0.5 \cdot 0.00017 \approx 0.000085
   $$


2. **选择最优词义**：  
   选择使后验概率最大的词义 \( s^* \)：  
   $$
   s^* = \arg\max_{s_k} \prod_{v_x \in C'} P(v_x|s_k) \cdot P(s_k)
   $$ 

因此，我们选择 $s_1$ 做为最终答案。

-----

### 总结
- **训练阶段**：通过统计方法估计条件概率 \( P(v_x|s_k) \) 和先验概率 \( P(s_k) \)。  
- **测试阶段**：利用独立性假设计算未见实例的后验概率，并选择最优词义。  
- **优点**：计算简单高效，适合大规模数据。  
- **局限性**：独立性假设可能不符合实际语言现象，影响模型性能。  

通过上述步骤，朴素贝叶斯方法能够有效地完成词义消歧任务。

-----

## Evaluation for WSD System

在评估一个词义消歧系统时，通常使用分类任务中常见的评估指标。以下是一个针对二分类（如正类和负类）的评估方法。

---

### 混淆矩阵（Contingency Table）
将系统的输出与黄金标准（Gold Standards）进行比较，生成如下混淆矩阵：  

|                      | **正类（Gold Standards）**   | **负类（Gold Standards）**   |
| -------------------- | ---------------------------- | ---------------------------- |
| **正类（系统输出）** | 真正例（True Positive, tp）  | 假正例（False Positive, fp） |
| **负类（系统输出）** | 假负例（False Negative, fn） | 真负例（True Negative, tn）  |

- **真正例（tp）**：系统正确预测为正类的样本数。  
- **假正例（fp）**：系统错误预测为正类的样本数。  
- **假负例（fn）**：系统错误预测为负类的样本数。  
- **真负例（tn）**：系统正确预测为负类的样本数。
  
![alt text](imagen.png ':size=60%')

-----

### 评估指标 

!> 这里的计算也是重点！

1. **准确率（Accuracy）**：  
   系统预测正确的样本占总样本的比例。  
   $$
   \text{Accuracy} = \frac{tp + tn}{tp + fp + fn + tn}
   $$

2. **精确率（Precision）**：  
   系统预测为正类的样本中，实际为正类的比例。  
   $$
   \text{Precision} = \frac{tp}{tp + fp}
   $$ 

3. **召回率（Recall）**：  
   实际为正类的样本中，被系统正确预测为正类的比例。  
   $$
   \text{Recall} = \frac{tp}{tp + fn}
   $$  
![alt text](image.png ':size=60%')
4. **F值（F-measure）**：  
   精确率和召回率的加权调和平均值，用于平衡两者。通常使用 \( F_1 \) 值（\( \beta = 1 \)）：  
   $$
   F_\beta = \frac{(1 + \beta^2) \cdot \text{Precision} \cdot \text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}}
   $$  
   当 \( \beta = 1 \) 时：  
   $$
   F_1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
   $$

-----

**Micro F1 和 Macro F1**

Micro-F1 和 Macro-F1 是两种常用的多类别分类任务评估指标，它们的核心区别在于对各类别的统计方式不同：

**Micro-F1**  

- **计算方式**：  
  - 将所有类别的预测结果（真正例 \( \text{tp} \)、假正例 \( \text{fp} \)、假负例 \( \text{fn} \)）**全局汇总**，然后计算整体的精确率（Precision）和召回率（Recall），最后计算 F1 值。  
  - 公式：  
    $$
    \text{Micro-Precision} = \frac{\sum_{i=1}^K \text{tp}_i}{\sum_{i=1}^K \text{tp}_i + \sum_{i=1}^K \text{fp}_i}
    $$
    $$
    \text{Micro-Recall} = \frac{\sum_{i=1}^K \text{tp}_i}{\sum_{i=1}^K \text{tp}_i + \sum_{i=1}^K \text{fn}_i}
    $$
    $$
    \text{Micro-F1} = \frac{2 \cdot \text{Micro-Precision} \cdot \text{Micro-Recall}}{\text{Micro-Precision} + \text{Micro-Recall}}
    $$

- **特点**：  
  - 考虑所有类别的预测结果，**偏向于数量较多的类别**。  
  - 适合类别不平衡的数据集，因为数量多的类别对结果影响更大。  

**Macro-F1**  
- **计算方式**：  
  - 对**每个类别单独计算**精确率（Precision）和召回率（Recall），然后对所有类别的 F1 值取**平均**。  
  - 公式：  
    $$
    \text{Macro-Precision} = \frac{1}{K} \sum_{i=1}^K \text{Precision}_i
    $$
    $$
    \text{Macro-Recall} = \frac{1}{K} \sum_{i=1}^K \text{Recall}_i
    $$
    $$
    \text{Macro-F1} = \frac{1}{K} \sum_{i=1}^K \text{F1}_i
    $$

- **特点**：  
  - 对所有类别**平等对待**，不考虑类别的样本数量。  
  - 适合类别平衡的数据集，或者当每个类别的重要性相同时。  

**核心区别**

| **特性**     | **Micro-F1**               | **Macro-F1**                         |
| ------------ | -------------------------- | ------------------------------------ |
| **统计方式** | 全局汇总，所有类别一起计算 | 对每个类别单独计算后取平均           |
| **类别权重** | 偏向数量多的类别           | 对所有类别平等对待                   |
| **适用场景** | 类别不平衡的数据集         | 类别平衡的数据集，或每个类别同等重要 |

-----

**示例**

假设有一个三分类任务，类别 A、B、C 的统计结果如下：  

| 类别 | tp  | fp  | fn  | Precision | Recall | F1     |
| ---- | --- | --- | --- | --------- | ------ | ------ |
| A    | 10  | 5   | 2   | 0.6667    | 0.8333 | 0.7407 |
| B    | 20  | 10  | 5   | 0.6667    | 0.8000 | 0.7273 |
| C    | 5   | 15  | 3   | 0.2500    | 0.6250 | 0.3571 |

- **Micro-F1**：  
  - 全局统计：\( \text{tp}_{\text{total}} = 35 \), \( \text{fp}_{\text{total}} = 30 \), \( \text{fn}_{\text{total}} = 10 \)  
  - Micro-Precision = \( \frac{35}{35 + 30} = 0.5385 \)  
  - Micro-Recall = \( \frac{35}{35 + 10} = 0.7778 \)  
  - Micro-F1 = \( \frac{2 \cdot 0.5385 \cdot 0.7778}{0.5385 + 0.7778} = 0.6341 \)  

- **Macro-F1**：  
  - 对每个类别的 F1 取平均：\( \text{Macro-F1} = \frac{0.7407 + 0.7273 + 0.3571}{3} = 0.6084 \)  

-----

> **总结各种评估方式**
> - **准确率**：衡量整体预测的准确性，但在类别不平衡时可能不够全面。  
> - **精确率**：关注系统预测的正类中有多少是正确的。  
> - **召回率**：关注实际的正类中有多少被系统正确预测。  
> - **F值**：综合精确率和召回率，是评估分类系统性能的重要指标。  

通过混淆矩阵和上述指标，可以全面评估词义消歧系统的性能，并根据任务需求优化模型。

-----

### 评估设置

在词义消歧（WSD）任务中，评估设置是衡量模型性能的重要环节。常见的评估设置包括**词汇样本任务（Lexical-Sample）**和**全词任务（All-Words）**，它们的主要区别在于目标词的范围和处理方式。

**词汇样本任务（Lexical-Sample / Targeted WSD）**

- **特点**：  
  - 针对**限定的一组目标词**进行评估。  
  - 通常每个句子中只包含**一个目标词**，任务是为该目标词选择正确的词义。  
  - 目标词通常是多义词，且其词义清单（sense inventory）已知。  

- **优点**：  
  - 任务定义明确，易于实现和评估。  
  - 适合研究特定目标词的词义消歧性能。  

- **缺点**：  
  - 仅关注少量目标词，无法全面反映模型在实际文本中的表现。  

**全词任务（All-Words）**  

- **特点**：  
  - 对给定文本中**所有开放类词汇**（包括名词、动词、形容词和副词）进行词义消歧。  
  - 需要为每个词训练单独的模型或分类器，因此任务复杂度较高。  

- **优点**：  
  - 更接近实际应用场景，能够全面评估模型在真实文本中的性能。  

- **缺点**：  
  - 需要训练大量分类器，计算成本高。  
  - 数据标注和模型训练的难度较大。  

| **评估设置**       | **词汇样本任务（Lexical-Sample）** | **全词任务（All-Words）**                  |
| ------------------ | ---------------------------------- | ------------------------------------------ |
| **目标词范围**     | 限定的一组目标词                   | 所有开放类词汇（名词、动词、形容词、副词） |
| **句子中的目标词** | 通常每句一个目标词                 | 每句多个目标词                             |
| **复杂度**         | 较低，适合研究特定目标词           | 较高，需要训练大量分类器                   |
| **适用场景**       | 针对特定目标词的性能评估           | 全面评估模型在实际文本中的表现             |

选择合适的评估设置取决于任务目标：如果关注特定目标词的性能，可以选择词汇样本任务；如果需要全面评估模型在实际文本中的表现，则应选择全词任务。

> **Basic Pipeline**
> 1. 定义任务  
> 2. 明确动机  
> 3. 做出合理假设  
> 4. 设计模型  
> 5. 设置评估协议  
> 6. 准备数据  
> 7. 运行实验  
> 8. 分析结果  
> 9. 撰写报告

-----

## Limitations & Improvements
WSD还是有很多limitations的。例如：

1. **数据不足**：标注数据有限，尤其是低频词或新词。  
2. **语言演化**：语言不断变化，新词义和新用法层出不穷。  
3. **新语言处理**：面对全新语言时，缺乏资源和知识支持。  
4. **依赖假设**：许多方法基于简化假设（如独立性假设），可能与实际语言现象不符。

### Two Interesting Heuristics
1. **Most Frequent Sense (MFS)** ：选择目标词的最常见词义作为预测结果。
2. **One Sense per Discourse / Collocation (OSD)** ：假设一个文档中只有一个词义，选择该词义作为预测结果。

-----

### Lesk: A simple algorithm
Lesk 算法是一种简单的词义消歧方法，其核心思想是选择**词义定义（gloss）与输入上下文之间词汇重叠最多**的词义。例如，对于多义词“bank”，算法会计算每个词义定义（如“银行”或“河岸”）与上下文中词汇的重叠程度，选择重叠最多的词义作为结果。  

**扩展与改进**

1. **更灵活的评分函数**：引入更复杂的词汇相似度计算，而非简单的词重叠计数。  
2. **加权机制**：为不同词汇或上下文特征赋予权重，提升重要词汇的影响力。  
3. **更多资源**：结合外部知识库（如 WordNet）或语料库，丰富上下文和词义定义的信息。  

Lesk 算法简单高效，但可通过上述扩展进一步提升性能。

### It Makes Sense (IMS)
IMS 是一种传统的词义消歧方法，通过结合多种上下文特征来选择最合适的词义。其核心思想是利用目标词周围的丰富信息来推断词义。

**主要特征**

1. **词性标注（POS Tags of Surrounding Words）**：  
   - 利用目标词附近词语的词性信息（如名词、动词等）来辅助词义判断。  

2. **周围词（Surrounding Words）**：  
   - 分析目标词附近的词汇，捕捉上下文中的语义线索。  

3. **局部搭配（Local Collocations）**：  
   - 识别目标词与周围词的固定搭配或常见组合，例如短语或习惯用法。  

**评价**

- IMS 方法在传统词义消歧任务中表现优异，是传统方法中的**state-of-the-art（最先进）**。  
- 通过结合多种上下文特征，能够更准确地捕捉词义的语义信息。  

### 拓展内容
* Neural & Pre-training Methods
* Multilingual, Cross-lingual, Multimodal, Few-shot, ...
* Visual-WSD: Visual Word Sense Disambiguation
