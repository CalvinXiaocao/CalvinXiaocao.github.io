{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9\n",
    "AI Intro, Mar 25, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "dealing with classification\n",
    "\n",
    "### Binary classification\n",
    "only two labels: -1 & 1\n",
    "\n",
    "$sign(f(x)) = 1 or -1$\n",
    "\n",
    "Loss function: zero-one loss\n",
    "\n",
    "$L(f(X), y) = 0$ if $y f(x) \\geq 0$ ; otherwise, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### substitute loss 替代损失函数\n",
    "using sigmoid function:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "NOTE: $1 - \\sigma(x) = \\sigma(-x)$\n",
    "\n",
    "* substitute loss: $L(f(x_i), y_i) = \\log(1 + e^{-y_if(x_i)})$\n",
    "* Hinge Loss: $L(f(x_i), y_i) = \\max(0, 1 - y_if(x_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation\n",
    "if samples are independent, then we can use product to represent MLE.\n",
    "\n",
    "dealing with MLE -> dealing with log-likelihood (easy to calculate!)\n",
    "\n",
    "see slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "`softmax function`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
