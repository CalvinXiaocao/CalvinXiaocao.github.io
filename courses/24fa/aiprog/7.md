# Convolution & Pooling
Lecture 7, Programming in AI, 2024 Fall

## Fully Connected Layers
#### Pros
Expressive, can be easily implemented with GEMM

#### Cons
Requires huge amount of parmeters, has no translation invariance

## Convolution Layer
Slide a kernel with shape k Ã— k over the input feature map to produce a new feature map
### Boundary Effects
* Ignore the locations
* Pad the image with zeros
* Assume periodicity
* Reflect border

### Pooling

### Convolutions as Matrix Multiplication
#### Method 1: Change W into a matrix
But... Computing Backword pass is not efficient! How to compute the gradient of Ws?

#### Method 2: Change X into a matrix
A large dense matrix multiplies a small dense matrix (GEMM)

Explicit GEMM (cost a lot of memory)

#### Method 3: Implicit GEMM algorithm

## Pooling Layer
Max Pooling, backward(max)
