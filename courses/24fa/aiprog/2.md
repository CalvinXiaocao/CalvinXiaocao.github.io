# Parallel Programming
Programming in AI, 2024 Fall

## Reason for Parallel Programming
How to run tehe activation function efficiently?

### Increasing Clock efficiency
Frequency & Power

It takes too much power with high quality.

### Computing in Parallel

### CPU vs GPU
* CPU: Complex control circuit
  * Flexibility + Performance
* GPU: Simple control circuit
  * More circuit for parallel computation

### CUDA is Written in C with Extensions

### A Typical CUDA Program
#### What CPU does?
* allocates a block of memory on GPU
* copies data from CPU to GPU
* initiates launching kernels on GPU
* copies result back from GPU to CPU
  
#### What GPU does?
* efficiently launch a lot of kernels
* run kernels in parallel
* A kernel looks like a serial C program for a thread
* The GPU will run the kernel for many threads in parallel

## The first CUDA program
`Implement torch.nn.functional.relu`

```c++
float relu_cpu(float x) {
    return x > 0 ? x : 0;
}

for (int i = 0; i < N; ++i) {
    h_out[i] = relu_cpu(h_in[i]);
}
```

#### On GPU?

```c++
__global__ void relu_gpu(float *in, float *out) {
    int i = threadIdx.x;
    out[i] = in[i] > 0 ? in[i] : 0;
}

relu_gpu<<<1, N>>>(d_in, d_out)
```

```c++
const int N = 64;
const int size = N * sizeof(float);

// allocate memory on CPU
float *h_in = (float*)malloc(size);
float *h_out = (float*)malloc(size);

// relu on CPU
for (int i = 0; i < N; ++i) {
    h_out[i] = relu_cpu(h_in[i]);
}
// free memory ...
```

```cpp
// 1.allocate memory on GPU
float *d_in = nullptr;
float *d_out = nullptr;
cudaMalloc(&d_in, size);
cudaMalloc(&d_out, size);

cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice);

relu_gpu<<<1, N>>>(d_in, d_out);

cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost)
```

### Compile and Execute the code
* Install visual studio (windows) / gcc (Linux)
* Install CUDA on a computer with Nvidia GPUs
* Set the environment variables

```bash
nvcc --version
```
* compile the code
`abc.cu`

### Configure the Kernel Launch
`relu_gpu<<<1, N>>>(d_in, d_out)`
* 1: number of blocks (can be dim3(a, b, c), but not recommended!)
* N: number of threads per block
* grid = block + thread
* Launch many blocks at once
* Maximum number of threads per block (256/512/1024)

```c++
// use 512 or 256 threads of block
const int kCudaThreadsNum = 512;
inline int CudaGetBlocks(const int N) {
    return (N + kCudaThreadsNum - 1) / kCudaThreadsNum;
}

// Define the grid stride looping
#define CUDA_KERNEL_LOOP(i, n) for (int i = blockIdx.x * blockDim.x + threadIdx.x; \
i < (n); \
i += blockDim.x * gridDim.x)
```

## GPU Memory & Hardware
### Tensor is Physically a Block of Memory
* size: (C, H, W)
* strides: (H*W, W, 1)
* dtype: float
* device: cuda:0

### GPU Memory Model

speed: local > shared > global > cpu memory
