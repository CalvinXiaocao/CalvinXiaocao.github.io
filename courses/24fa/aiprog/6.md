# Matrix Multiplication
Lecture 6, Programming in AI, 2024 Fall

## Matrix Product
Fully Connected Layers
$$
Y_{m \times n} = W_{m \times k} \times X_{k \times n} \\
\frac{\partial L}{\partial X}_{k \times n} = W^{T}_{m\times k} \times \frac{\partial L}{\partial Y}_{m \times n} \\
\frac{\partial L}{\partial W}_{m\times k} = \frac{\partial L}{\partial Y}_{m \times n} \times X^T_{k\times n}
$$

### Matrix Product on GPU
General matrix multiplication (gemm)
$$
C = \alpha A \times B + \beta C
$$

#### Naive Method
Each thread is responsible for an element in the matrix C

#### Sgemm_coalesce
Make sure the threads end up in same warp to exploit coalescing.

## Roofline Model
* Bandwidth bound
  * bound by how fast the data can be moved through the memory system
* Compute bound
  * Bound by how fast the computation can be done

## Matrix Product with Shared Memory
Use shared memory to increase speed

## Sparse Matrix

## Cuda linear algebra & math libraries
### Thrust
* Vectors
有STL库，有排序算法等

### cuBLAS
计算矩阵乘法、产生随机数

`Hint: 作业中可以将gemm进行封装`
